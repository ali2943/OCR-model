# PaddleOCR Fine-tuning Environment

A complete setup for fine-tuning PaddleOCR text recognition models with custom datasets. This repository provides an organized structure, dataset preparation utilities, training scripts, inference code, and a **FastAPI web service with HTML interface** for building and deploying custom OCR models.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Folder Structure](#folder-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Web Service](#web-service)
- [Dataset Preparation](#dataset-preparation)
- [Training Workflow](#training-workflow)
- [Configuration](#configuration)
- [Inference](#inference)
- [Monitoring & Troubleshooting](#monitoring--troubleshooting)
- [Resources](#resources)

## ğŸ¯ Overview

This setup enables you to:
- Fine-tune PaddleOCR recognition models on custom datasets
- Use the powerful SVTR_LCNet architecture with PP-OCRv3 pretrained weights
- Prepare datasets with automatic validation and splitting
- Monitor training progress and evaluate model performance
- Export trained models for production inference
- **Deploy OCR model as a web service with REST API**
- **Use a modern HTML interface for easy OCR processing**

## ğŸ“ Folder Structure

```
OCR-model/
â”œâ”€â”€ dataset/                      # Dataset directory
â”‚   â”œâ”€â”€ raw/                      # Raw dataset
â”‚   â”‚   â”œâ”€â”€ images/               # Raw images
â”‚   â”‚   â”œâ”€â”€ labels.txt            # Image-text pairs (TAB-separated)
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”œâ”€â”€ train/                    # Training set
â”‚   â”‚   â”œâ”€â”€ images/               # Training images
â”‚   â”‚   â”œâ”€â”€ train_list.txt        # Training labels (PaddleOCR format)
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”œâ”€â”€ val/                      # Validation set
â”‚   â”‚   â”œâ”€â”€ images/               # Validation images
â”‚   â”‚   â”œâ”€â”€ val_list.txt          # Validation labels
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”œâ”€â”€ test/                     # Test set
â”‚   â”‚   â”œâ”€â”€ images/               # Test images
â”‚   â”‚   â”œâ”€â”€ test_list.txt         # Test labels
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â””â”€â”€ dict.txt                  # Character dictionary (auto-generated)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ rec_custom.yml            # Training configuration
â”œâ”€â”€ pretrained_models/            # Pretrained model weights
â”‚   â”œâ”€â”€ en_PP-OCRv3_rec_train/    # PP-OCRv3 English model
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ output/                       # Training outputs
â”‚   â”œâ”€â”€ rec_model/                # Model checkpoints
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â””â”€â”€ inference/                # Exported inference models
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ scripts/                      # Helper scripts
â”‚   â”œâ”€â”€ prepare_dataset.py        # Dataset preparation
â”‚   â”œâ”€â”€ download_pretrained.sh    # Download pretrained models
â”‚   â”œâ”€â”€ train.sh                  # Training script
â”‚   â”œâ”€â”€ evaluate.sh               # Evaluation script
â”‚   â””â”€â”€ export.sh                 # Model export script
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ predict.py                # Inference script
â”œâ”€â”€ static/                       # Web interface
â”‚   â””â”€â”€ index.html                # HTML interface for OCR
â”œâ”€â”€ logs/                         # Training logs
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ app.py                        # FastAPI web service
â”œâ”€â”€ API_DOCS.md                   # API documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Git ignore rules
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Installation

### 1. System Requirements

- Python 3.8+
- CUDA 11.2+ (for GPU training)
- 8GB+ RAM (16GB+ recommended)
- 10GB+ free disk space

### 2. Clone Repository

```bash
git clone https://github.com/ali2943/OCR-model.git
cd OCR-model
```

### 3. Install Dependencies

For GPU (recommended):
```bash
pip install -r requirements.txt
```

For CPU only:
```bash
# Edit requirements.txt and replace 'paddlepaddle-gpu' with 'paddlepaddle'
pip install -r requirements.txt
```

### 4. Clone PaddleOCR

```bash
git clone https://github.com/PaddlePaddle/PaddleOCR.git
cd PaddleOCR
pip install -e .
cd ..
```

## âš¡ Quick Start

### Complete Workflow

```bash
# 1. Prepare your dataset (see Dataset Preparation section)
python scripts/prepare_dataset.py

# 2. Download pretrained model
bash scripts/download_pretrained.sh

# 3. Train the model
bash scripts/train.sh

# 4. Evaluate the model
bash scripts/evaluate.sh

# 5. Export for inference
bash scripts/export.sh

# 6. Run inference
python inference/predict.py path/to/test/image.jpg

# 7. Start the web service (NEW!)
python app.py
```

## ğŸŒ Web Service

### Starting the Web Server

After exporting your trained model, you can start the web service:

```bash
python app.py
```

This will start a FastAPI server on `http://localhost:8000` with:
- ğŸ–¥ï¸ **Web Interface**: User-friendly HTML interface at `http://localhost:8000`
- ğŸ“š **API Documentation**: Interactive docs at `http://localhost:8000/docs`
- ğŸ”Œ **REST API**: Programmatic access to OCR functionality

### Web Interface Features

The HTML interface (`http://localhost:8000`) provides:
- **Drag & Drop**: Easily upload images by dragging them into the browser
- **Live Preview**: See your uploaded image before processing
- **Instant Results**: Get OCR results displayed in real-time
- **Detailed Mode**: View confidence scores and bounding boxes
- **Responsive Design**: Works on desktop and mobile devices

### API Endpoints

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Single Image OCR:**
```bash
curl -X POST "http://localhost:8000/api/ocr" \
  -F "file=@image.jpg"
```

**Batch Processing:**
```bash
curl -X POST "http://localhost:8000/api/ocr/batch" \
  -F "files=@image1.jpg" \
  -F "files=@image2.jpg"
```

### Server Options

```bash
# Custom port
python app.py --port 5000

# CPU inference
python app.py --cpu

# Custom model path
python app.py --model_dir ./output/inference/ --dict_path ./dataset/dict.txt

# Development mode with auto-reload
python app.py --reload
```

For complete API documentation, see [API_DOCS.md](API_DOCS.md)

## ğŸ“Š Dataset Preparation

### Input Format

Your raw dataset should be organized as:

```
dataset/raw/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ img_001.png
â”‚   â”œâ”€â”€ img_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ labels.txt
```

**labels.txt format** (TAB-separated):
```
img_001.png	Ground truth text
img_002.png	Another text sample
img_003.png	More examples here
```

âš ï¸ **Important**: Use TAB (`\t`) character as separator, not spaces!

### Run Dataset Preparation

```bash
python scripts/prepare_dataset.py
```

This script will:
- âœ… Validate all images (check for corruption)
- âœ… Split data into train/val/test (80%/10%/10%)
- âœ… Convert to PaddleOCR format
- âœ… Generate character dictionary
- âœ… Provide detailed statistics

**Optional arguments:**
```bash
python scripts/prepare_dataset.py \
    --raw_dir ./dataset/raw \
    --output_dir ./dataset \
    --train_ratio 0.8 \
    --val_ratio 0.1 \
    --seed 42
```

### Output Format

After preparation, you'll have:

**train_list.txt / val_list.txt / test_list.txt:**
```
images/img_001.png	Ground truth text
images/img_002.png	Another text sample
```

**dict.txt** (one character per line):
```
a
b
c
...
```

## ğŸ“ Training Workflow

### 1. Download Pretrained Model

```bash
bash scripts/download_pretrained.sh
```

Downloads PP-OCRv3 English recognition model (~50MB).

### 2. Start Training

```bash
bash scripts/train.sh
```

The script will:
- Check for PaddleOCR installation
- Validate configuration and data paths
- Start training with progress monitoring
- Save checkpoints every 10 epochs
- Evaluate during training

**Training parameters** (in `configs/rec_custom.yml`):
- Epochs: 100
- Batch size: 128
- Learning rate: 0.001 (Cosine scheduler)
- Image shape: [3, 48, 320]
- Architecture: SVTR_LCNet

### 3. Monitor Training

Watch the terminal output for:
- Loss values (should decrease)
- Accuracy metrics
- Learning rate schedule

Checkpoints are saved to `output/rec_model/`:
- `best_accuracy.pdparams` - Best performing model
- `latest.pdparams` - Most recent checkpoint
- `iter_epoch_*.pdparams` - Periodic checkpoints

### 4. Evaluate Model

```bash
bash scripts/evaluate.sh
```

Evaluates on validation set and reports:
- Character-level accuracy
- Word-level accuracy
- Per-sample predictions

### 5. Export for Inference

```bash
bash scripts/export.sh
```

Exports the model to `output/inference/`:
- `inference.pdmodel` - Model architecture
- `inference.pdiparams` - Model weights

## âš™ï¸ Configuration

### Main Configuration File: `configs/rec_custom.yml`

**Key settings you can customize:**

```yaml
Global:
  epoch_num: 100                    # Training epochs
  save_epoch_step: 10               # Checkpoint frequency
  eval_batch_step: 500              # Evaluation frequency
  character_dict_path: ./dataset/dict.txt
  max_text_length: 25               # Maximum text length

Optimizer:
  lr:
    learning_rate: 0.001            # Initial learning rate
    warmup_epoch: 5                 # Warmup epochs

Train:
  loader:
    batch_size_per_card: 128        # Batch size
    num_workers: 8                  # Data loading workers

Eval:
  loader:
    batch_size_per_card: 128
    num_workers: 4
```

### Image Shape

The default image shape is `[3, 48, 320]` (channels, height, width). Adjust based on your data:
- Taller images â†’ increase height (e.g., 64)
- Longer text â†’ increase width (e.g., 384, 512)

## ğŸ”® Inference

### Single Image Prediction

```bash
python inference/predict.py path/to/image.jpg
```

### Detailed Output (with confidence scores)

```bash
python inference/predict.py path/to/image.jpg --detail
```

### Batch Prediction

```bash
python inference/predict.py --batch img1.jpg img2.jpg img3.jpg
```

### CPU Inference

```bash
python inference/predict.py path/to/image.jpg --cpu
```

### Python API

```python
from inference.predict import CustomPaddleOCR

# Initialize
ocr = CustomPaddleOCR(
    model_dir='./output/inference/',
    dict_path='./dataset/dict.txt',
    use_gpu=True
)

# Single prediction
text = ocr.predict('image.jpg')
print(f"Recognized: {text}")

# Detailed prediction
results = ocr.predict('image.jpg', detail=True)
for item in results:
    print(f"Text: {item['text']}, Confidence: {item['confidence']}")

# Batch prediction
texts = ocr.predict_batch(['img1.jpg', 'img2.jpg'])
```

## ğŸ” Monitoring & Troubleshooting

### Common Issues

**1. Out of Memory (OOM)**
- Reduce `batch_size_per_card` in config
- Reduce `num_workers`
- Use smaller images

**2. PaddleOCR not found**
```bash
cd PaddleOCR
pip install -e .
```

**3. Dataset preparation fails**
- Check `labels.txt` format (TAB-separated)
- Verify image file paths
- Ensure images are valid (not corrupted)

**4. Training loss not decreasing**
- Check learning rate (try 0.0001 or 0.01)
- Verify data quality
- Increase training epochs
- Try different batch sizes

**5. Low accuracy**
- Increase training epochs
- Add more training data
- Adjust image preprocessing
- Fine-tune hyperparameters

### Training Tips

1. **Start with pretrained model**: Always use pretrained weights for better results
2. **Monitor validation accuracy**: Stop if validation accuracy stops improving
3. **Data quality matters**: Clean, accurate labels are crucial
4. **Experiment with batch size**: Larger batches â†’ more stable training
5. **Use GPU**: Training on CPU is extremely slow

### Log Files

- Training logs: Check terminal output or redirect to file
- Model checkpoints: `output/rec_model/`
- Predictions: `output/rec_model/predicts.txt`

## ğŸ“š Resources

### PaddleOCR Documentation

- [Official Documentation](https://github.com/PaddlePaddle/PaddleOCR)
- [Model Zoo](https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/models_list_en.md)
- [Training Guide](https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/recognition_en.md)
- [Configuration Docs](https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/config_en.md)

### Tutorials

- [Text Recognition Tutorial](https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/recognition_en.md)
- [Custom Dataset Guide](https://github.com/PaddlePaddle/PaddleOCR/blob/main/doc/doc_en/dataset/recognition_dataset_en.md)

### Support

- [PaddleOCR Issues](https://github.com/PaddlePaddle/PaddleOCR/issues)
- [PaddlePaddle Forum](https://github.com/PaddlePaddle/Paddle/discussions)

## ğŸ“ Dataset Format Examples

### Example 1: Simple Text Recognition

**labels.txt:**
```
receipt_001.jpg	$45.99
receipt_002.jpg	Total: $123.45
invoice_003.jpg	Invoice #12345
```

### Example 2: License Plates

**labels.txt:**
```
plate_001.jpg	ABC-1234
plate_002.jpg	XYZ-5678
plate_003.jpg	DEF-9012
```

### Example 3: Document Text

**labels.txt:**
```
doc_001.jpg	Machine Learning Research
doc_002.jpg	Annual Report 2023
doc_003.jpg	Project Proposal
```

## ğŸ› ï¸ Advanced Usage

### Custom Architecture

Edit `configs/rec_custom.yml` to try different architectures:
- CRNN
- RARE
- SRN
- NRTR
- SAR

### Data Augmentation

The configuration includes `RecAug` for automatic augmentation:
- Random rotation
- Color jittering
- Gaussian noise
- Perspective transformation

Adjust in config or add custom augmentation in the pipeline.

### Multi-GPU Training

```bash
# In PaddleOCR directory
python -m paddle.distributed.launch \
    --gpus '0,1,2,3' \
    tools/train.py -c ../configs/rec_custom.yml
```

## ğŸ“„ License

This project structure is based on PaddleOCR, which is licensed under Apache License 2.0.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

---

**Happy Training! ğŸš€**

For questions or issues, please open an issue on GitHub.
